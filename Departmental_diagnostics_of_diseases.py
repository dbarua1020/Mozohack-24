# -*- coding: utf-8 -*-
"""Medical Diagnostics System ðŸ”¬ðŸ’™

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/medical-diagnostics-system-5958ecbf-ff9b-49ae-81be-34f6d5e75af8.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240329/auto/storage/goog4_request%26X-Goog-Date%3D20240329T191447Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D84d23080762baf9ebb2aaee62ca69539f3a105669695fd30ca6b9b2f94dcb89bbb057cafb056e92aa223647d4c840711e3150a641c3dd48f22063458fd352854df674c3d9c9c2c56a85e2a322c8a5a0c68ec34adfa6cb29e97f47e1c90df01c5969f14f18039452ac726154d3d9e1a647c5e3a8f0cfb1ee3cd981045a303f527d0f8211b354ed5aae23813fbcfc9d17512da93bd2a0356018d415c771c1100d5b72129aa9f44620fb05126496e8aa395e54d76bbfa3099de45450e8408b8b008991619a60231c939ca42bea2b7dc2ce8441e0913ebf823eee3cb3ed8380bad01fd5cff8f80f2258e2dceed0dba2df34d65b93de0776aeff0dae2deeb1545459e
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'breast-cancer-wisconsin-data:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F180%2F408%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240329%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240329T191446Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3e5724f50a17425c7a9afdb23ea8db111611ed76954c4235c6ae8eb1b422d226709d96e2448561472b7c9224c367895915305d2e5af19f96b90014c11800cb01350ab1ae415d1d2ebb7df52e73878cdf7348b61fab8cef55e65165996d9f42b3acd16df8a35e21311aea5c22603539fd5c36a85887847a7cb79ba066fda0c55109090cb111161c43da1ed1ac85d0657bb924cba6c173e21e253456ae370fba3b42b3b0bba7f64073d889e18f709f4d7d988680b32d0297141ad5a1839db322c27d1d11ec776941ca7aeeee6f5c8961adadf1a602472422dd4d0e8ab86bbf21b9dd56bff2f4b88b1ed410a2e4f15442e99e2b4b356f90d2070bc246615603ace7,pima-indians-diabetes-database:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F228%2F482%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240329%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240329T191446Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8806b71b3b2046ab339d6d4c6cd41358fcfe567ab19c807e4b9e9a5ac9a9eee31ba129f4931192b4a4607beeca1b34d65fbd06f3784e2ca4ee81f8de7e0563ba63b73a5188237945fa03a5aca5406a21f0a94590ca4227abc42336ca11e52864b7ee7c974754b180b9204f02a8b00d5048559e8a88ea8715ae3b3679176cdc2b1555edb50be3930d17019f3f5bcf22557fb56139b97c19ff0524ea7bb19b7a0990fe01e5b7a2969e262574f3e6ca573c183cfdda61acf50a22cbb216a6ff2e216cc03e2e122f9a2fb6e2173c046f66ccbf056ac30ab95d3ff53a9bf6f542adefd8692298a7af95f51878cb3e9c7f8d864fcd6bc19c91128d88548045f46fbe75,ckdisease:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1111%2F2005%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240329%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240329T191447Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D358a28329d16fde3a725efa76c22ba57cbc3005d9ac55e1e1fc506593977c5a698ce87dc6bc554468a386b862995864b062f8fa5b7b60ead2da9d01ec2e57c2b60dd7bfc1af1764b9af0446f4c5b4a4f32bb25fb190525594fd7f13b743d08bc50ccd157803c089add39b1475bdb3c240aef217d6c249bb9f0ea4884961dde5b235a9b40f93145064437d268baf75c4116c6ebc60f6b29c1efe0e18ac5d63ed12813094122b7b31fe284f1eca64ede87824b1ea37e51664bae295277ec11fe733c3a087bb6c8be9bffa1ca363fbd3e836dd69780989be02ea6538654e7460de7521a5c0fce886a67c54459ba9f0cca46576cf32e0792516412c052378f391099,indian-liver-patient-records:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2607%2F4342%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240329%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240329T191447Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D34ffd926f7b66f27acf50489c0934e8c8af0866c581994da6076aa9fb308b7305a55fb81269ada6abb1c1625d34b03b3f1c7c53cc68a7bc1d95c6019b140a5aa541109397d48a85c187cc47b67e07247a2e00d948816e0b25556d9dacdf03148e3e45a4cf8352456346c20eac60f656e5f0d8d0fa2d4cd38c9f78fe93bd95f0c559fc6cebbf926a2b16c629ed8dad175aad176405d55baeffec0cba3651e68bd2c0d4a47831f88605f0d1008944dd0ea343de3b074605ca5484344ce91ad10cef9026703f92cd2bb663204fc7c7b845675a5b39a1aa4f3ed776f5c742ac064685ee378151295392b6c52940085bb3b6cd6fbd800a769f6f2a3ac71296fa16cdf,uci-heart-disease-data:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2931242%2F6003299%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240329%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240329T191447Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1b53d52d2c7d5c1ba989558844098dfdbc68ec6c7ecbd1987cf56aad62c1305729aa93d848467338ec6a16e6cd08c84a212b6772fee21c585d3822f05d2db83b497f999cd7337ad97c26544b89ef79082da841d2e7b94938e44d9907a069623afbcb7a663a82950f49b2c7e40a357f4a5a30f7d56281f7e15663ced0b8029d57965f1d16af0aba6224eb4d05c69ccd930d35036afb57814044b8844df01b9ad2fb6db39d36d1e7ea07d1150a13a41138c4a9789207c054c683caee5289b1ce2a8ad9e0507f11bf11da56c8bf3e6edecdfe7cd95326a10d603f1907566c92da48e386e9745c9929eae75e357c314bf36bb104467de4ca9cef66e5626b20c52167'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""<div style="background-color:#3b5998; border-radius:2px; border:#000000 solid; padding: 15px; font-size:100%; text-align:center">
    <h1 align="center" style="color:#ffffff;"><b>MEDICAL DIAGNOSTICS SYSTEM ðŸ”¬ðŸ’™</b></h1>
</div>

<div style="background-color:#ffffff; border-radius:2px; border:#000000 solid; padding: 15px; font-size:100%; text-align:center">
    <img src="https://media.tenor.com/RUzlE5PNjgYAAAAC/gloves-on-im-ready.gif" alt="Animated GIF">
</div>

<div style="background-color:#dfe3ee; border-radius:2px; border:#000000 solid; padding: 15px; font-size:100%; text-align:center">
    <h2 align="center" style="color:#000000;"><b>IMPORT DEPENDENCIES</b></h2>
</div>
"""

import joblib
import logging
import warnings

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from scipy.stats import skew
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import (
    StandardScaler,
    MinMaxScaler,
    MaxAbsScaler,
    RobustScaler,
    PowerTransformer,
    QuantileTransformer,
    Normalizer,
    Binarizer,
)
from sklearn.linear_model import (
    LogisticRegression,
    SGDClassifier,
    ElasticNet,
    RidgeClassifier,
    Perceptron,
)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    AdaBoostClassifier,
    BaggingClassifier,
    ExtraTreesClassifier,
)
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.mixture import GaussianMixture
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report
from sklearn.preprocessing import LabelEncoder

# Ignore warnings
warnings.filterwarnings("ignore")

# Set a random seed
random_seed = 42
np.random.seed(random_seed)

diabetes_data = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')
cancer_data   = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')
heart_data    = pd.read_csv('/kaggle/input/uci-heart-disease-data/heart_disease_data.csv')
chronic_data  = pd.read_csv('/kaggle/input/ckdisease/kidney_disease.csv')
liver_data    = pd.read_csv('/kaggle/input/indian-liver-patient-records/indian_liver_patient.csv')

def plot_dataframe_distributions(df, ncols=3, nrows=3, figsize=(20, 10)):
    num_cols = len(df.columns)
    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)
    axes = axes.flatten()[:num_cols]

    for i, (col, ax) in enumerate(zip(df.columns, axes)):
        sns.histplot(data=df, x=col, kde=True, ax=ax)
        ax.set_title(col)

    if num_cols < len(axes):
        for j in range(num_cols, len(axes)):
            fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

def plot_outliers(data):
    plt.figure(figsize=(12, 8))
    ax = sns.boxplot(data=data)
    plt.title('Outlier Detection')
    ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='center')
    plt.show()

def plot_correlation_heatmap(data):
    correlation = data.corr()
    plt.figure(figsize=(18, 12))
    sns.heatmap(correlation, cmap='BrBG', annot=True)
    plt.title('Correlation Heatmap')
    plt.show()

"""<div style="background-color:#dfe3ee; border-radius:2px; border:#000000 solid; padding: 15px; font-size:100%; text-align:center">
    <h2 align="center" style="color:#000000;"><b>_ Diabetes Disease _</b></h2>
</div>
"""

diabetes_data.head()

diabetes_data.info()

diabetes_data.describe().T  # Generate descriptive statistics of the data

diabetes_data.columns = ['pregnancies','glucose','blood_pressure','skin_thickness','insulin','bmi','diabetes_pedigree_function','age','outcome']

plot_dataframe_distributions(diabetes_data)

plot_outliers(diabetes_data)

# Remove Outliers
Q1 = diabetes_data.quantile(0.25)   # Compute the first quartile (25th percentile)
Q3 = diabetes_data.quantile(0.75)   # Compute the third quartile (75th percentile)
IQR = Q3 - Q1             # Compute the interquartile range (IQR)
print(diabetes_data.shape)
# Remove the outliers
# Exclude rows where any feature value is below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR
diabetes_data = diabetes_data[~((diabetes_data < (Q1 - 1.5 * IQR)) | (diabetes_data > (Q3 + 1.5 * IQR))).any(axis=1)]
print(diabetes_data.shape)

plot_correlation_heatmap(diabetes_data)

diabetes_data['outcome'].value_counts()

X = diabetes_data.drop(columns=['outcome'])
Y = diabetes_data['outcome']

# Random oversamplingrandom_state=random_seed
oversampler = RandomOverSampler(random_state=random_seed)
X_resampled, Y_resampled = oversampler.fit_resample(X,Y)

Y_resampled.value_counts()

x_train, x_test, y_train, y_test = train_test_split(X_resampled,Y_resampled,test_size=0.2,random_state=random_seed)

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

# Set the log level to suppress the text output
logging.getLogger("xgboost").setLevel(logging.WARNING)
logging.getLogger("catboost").setLevel(logging.WARNING)

# Define classifiers and their corresponding parameters
classifiers = [
    LogisticRegression(random_state=random_seed),
    KNeighborsClassifier(),
    SVC(random_state=random_seed),
    DecisionTreeClassifier(random_state=random_seed),
    RandomForestClassifier(random_state=random_seed),
    GradientBoostingClassifier(verbose=0,random_state=random_seed),
    AdaBoostClassifier(random_state=random_seed),
    GaussianNB(),
    MLPClassifier(random_state=random_seed),
    XGBClassifier(verbosity=0,random_state=random_seed),
    ExtraTreesClassifier(random_state=random_seed),
]

# Define scalers
scalers = [
    StandardScaler(),
    MinMaxScaler(),
    MaxAbsScaler(),
    RobustScaler(),
    PowerTransformer(),
    PowerTransformer(),
    QuantileTransformer(),
    Normalizer(),
    Binarizer()
]

results = []
for scaler in scalers:
    for classifier in classifiers:
        # Create a pipeline with the scaler and classifier
        pipeline = Pipeline([
            ('scaler', scaler),
            ('classifier', classifier)
        ])

        # Perform cross-validation using the pipeline
        scores = cross_val_score(pipeline, X_resampled, Y_resampled, cv=5, scoring='accuracy');

        # Calculate additional evaluation metrics
        y_pred = cross_val_predict(pipeline, X_resampled, Y_resampled, cv=5)
        accuracy = accuracy_score(Y_resampled, y_pred)
        recall = recall_score(Y_resampled, y_pred, average='weighted')
        precision = precision_score(Y_resampled, y_pred, average='weighted')
        f1 = f1_score(Y_resampled, y_pred, average='weighted')

        # Append the results to the list
        results.append((classifier.__class__.__name__, scaler.__class__.__name__, accuracy, recall, precision, f1))

results = pd.DataFrame(results, columns=['Classifier', 'Scaler', 'Accuracy', 'Recall', 'Precision', 'F1-Score'])
results = results.sort_values(by='Recall', ascending=False)
results.head()

model_pipeline = Pipeline([
    ('scaler',MaxAbsScaler()),
    ('classifier',ExtraTreesClassifier())
])
model_pipeline.fit(x_train,y_train)

y_pred = model_pipeline.predict(x_test)
print(classification_report(y_test, y_pred))

joblib.dump(model_pipeline,'diabetes_model.sav')
results.to_csv('diabetes_results.csv',index=False)

"""<div style="background-color:#dfe3ee; border-radius:2px; border:#000000 solid; padding: 15px; font-size:100%; text-align:center">
    <h2 align="center" style="color:#000000;"><b>_ Breast Cancer Disease _</b></h2>
</div>
"""

cancer_data.head()

cancer_data.info() # information about the data

cancer_data.describe().T

# Remove the 'id' and 'Unnamed: 32' columns from the 'data' DataFrame
cancer_data.drop(['id', 'Unnamed: 32'], axis=1,inplace=True)
cancer_data.head()

# Calculate the number of null values in each column
cancer_data.isna().sum()

cancer_data['diagnosis'].value_counts()

cancer_data['diagnosis'].replace({'B':0,'M':1},inplace=True)
cancer_data.head()

plot_correlation_heatmap(cancer_data)

correlation = cancer_data.corr()
correlation['diagnosis']

corrgt5 = correlation[abs(correlation['diagnosis']) > 0.5]  # Select columns with correlation greater than 0.5
corrgt5 = corrgt5.index  # Get the index (column names) of the selected columns

print(corrgt5)  # Print the selected column names
print()
print("Highly correlated features:", len(corrgt5))  # Print the number of highly correlated features

corrlt5 = correlation[abs(correlation['diagnosis']) <= 0.5]  # Select columns with correlation less than or equal to 0.5
corrlt5 = corrlt5.index  # Get the index (column names) of the selected columns

print(corrlt5)  # Print the selected column names
print()
print("Least correlated features:", len(corrlt5))  # Print the number of least correlated features

cancer_data = cancer_data[[
    'diagnosis', 'radius_mean', 'area_mean','compactness_mean', 'concavity_mean',
    'concave points_mean','perimeter_worst', 'area_worst', 'compactness_worst',
    'concavity_worst', 'concave points_worst', 'texture_mean', 'smoothness_mean',
    'symmetry_mean', 'area_se', 'fractal_dimension_se','texture_worst', 'smoothness_worst',
    'symmetry_worst','fractal_dimension_worst'
]]
cancer_data.head()

cancer_data['diagnosis'].value_counts()

# Create histogram and kernel density estimate (KDE) plots to observe the data distribution
fig, ax = plt.subplots(ncols=5, nrows=4, figsize=(40,30))  # Create subplots with specified size
index = 0  # Initialize the index variable
ax = ax.flatten()  # Flatten the subplots array

# Iterate over each column and its corresponding values in the dataset
for col, value in cancer_data.items():
    col_dist = sns.histplot(value, ax=ax[index], color='green', kde=True, stat="density", linewidth=0)  # Create a histogram plot with KDE
    col_dist.set_xlabel(col,fontsize=18)  # Set the x-axis label as the column name
    col_dist.set_ylabel('density',fontsize=18)  # Set the y-axis label as 'density'
    index += 1  # Increment the index for the next subplot

plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)  # Adjust the spacing and padding between subplots

# Selecting only the skewed columns to transform them into a Gaussian distribution
X = cancer_data[['radius_mean','area_mean','compactness_mean','concavity_mean',
                 'concave points_mean','area_worst', 'compactness_worst',
                 'concavity_worst', 'area_se','fractal_dimension_se',
                 'symmetry_worst','fractal_dimension_worst'
]]

print(X.shape)
X.head()

# Create hist and kde plots to observe the data distribution
fig, ax = plt.subplots(ncols=4, nrows=3, figsize=(40,30))  # Create subplots with specified number of columns and rows
index = 0  # Initialize the index counter
ax = ax.flatten()  # Flatten the axes array to iterate over them

for col, value in X.items():
    col_dist = sns.histplot(value, ax=ax[index], color='green', kde=True, stat="density", linewidth=0)
    col_dist.set_xlabel(col, fontsize=18)  # Set x-axis label as the column name
    col_dist.set_ylabel('density', fontsize=18)  # Set y-axis label as 'density'
    index += 1  # Increment the index counter

plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)  # Adjust the subplot layout

for feature in X:
    upper_limit = X[feature].mean() + 3 * X[feature].std()
    lower_limit = X[feature].mean() - 3 * X[feature].std()
    X[feature] = np.where(X[feature] > upper_limit, upper_limit,np.where(X[feature] < lower_limit, lower_limit,X[feature]))

fig, ax = plt.subplots(ncols=4, nrows=3, figsize=(40, 30))
index = 0
ax = ax.flatten()

for col, value in X.items():
    col_dist = sns.histplot(value, ax=ax[index], color='green', kde=True, stat="density", linewidth=0)
    col_dist.set_xlabel(col, fontsize=18)
    col_dist.set_ylabel('density', fontsize=18)

    index += 1

plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)

print(X.columns)
print(X.shape)

Y = cancer_data['diagnosis']

Y.value_counts()

# Random oversampling
oversampler = RandomOverSampler(random_state=random_seed)
X_resampled, Y_resampled = oversampler.fit_resample(X,Y)

Y_resampled.value_counts()

x_train, x_test, y_train, y_test = train_test_split(X_resampled,Y_resampled,test_size=0.2, random_state=42)

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

logging.getLogger("xgboost").setLevel(logging.WARNING)
logging.getLogger("catboost").setLevel(logging.WARNING)


classifiers = [
    LogisticRegression(random_state=random_seed),
    KNeighborsClassifier(),
    SVC(random_state=random_seed),
    DecisionTreeClassifier(random_state=random_seed),
    RandomForestClassifier(random_state=random_seed),
    GradientBoostingClassifier(verbose=0,random_state=random_seed),
    AdaBoostClassifier(random_state=random_seed),
    GaussianNB(),
    MLPClassifier(random_state=random_seed),
    XGBClassifier(verbosity=0,random_state=random_seed),
    ExtraTreesClassifier(random_state=random_seed),
]


scalers = [
    StandardScaler(),
    MinMaxScaler(),
    MaxAbsScaler(),
    RobustScaler(),
    PowerTransformer(),
    PowerTransformer(),
    QuantileTransformer(),
    Normalizer(),
    Binarizer()
]

results = []
for scaler in scalers:
    for classifier in classifiers:

        pipeline = Pipeline([
            ('scaler', scaler),
            ('classifier', classifier)
        ])


        scores = cross_val_score(pipeline, X_resampled, Y_resampled, cv=5, scoring='accuracy');


        y_pred = cross_val_predict(pipeline, X_resampled, Y_resampled, cv=5)
        accuracy = accuracy_score(Y_resampled, y_pred)
        recall = recall_score(Y_resampled, y_pred, average='weighted')
        precision = precision_score(Y_resampled, y_pred, average='weighted')
        f1 = f1_score(Y_resampled, y_pred, average='weighted')


        results.append((classifier.__class__.__name__, scaler.__class__.__name__, accuracy, recall, precision, f1))

results = pd.DataFrame(results, columns=['Classifier', 'Scaler', 'Accuracy', 'Recall', 'Precision', 'F1-Score'])
results = results.sort_values(by='Recall', ascending=False)
results.head()

model_pipeline = Pipeline([
    ('scaler',Normalizer()),
    ('classifier',ExtraTreesClassifier(verbose=0))
])
model_pipeline.fit(x_train,y_train)

y_pred = model_pipeline.predict(x_test)
print(classification_report(y_test,y_pred))

joblib.dump(model_pipeline,'cancer_model.sav')
results.to_csv('cancer_results.csv',index=False)

"""<div style="background-color:#dfe3ee; border-radius:2px; border:#000000 solid; padding: 15px; font-size:100%; text-align:center">
    <h2 align="center" style="color:#000000;"><b>_ Heart Disease _</b></h2>
</div>
"""

heart_data.head()

heart_data.info()

# checking for missing values
heart_data.isnull().sum()

heart_data.describe().T

plot_dataframe_distributions(heart_data,ncols=4,nrows=3)

plot_correlation_heatmap(heart_data)

heart_data['target'].value_counts()

X = heart_data.drop(columns=['target'])
Y = heart_data['target']

# Random oversampling
oversampler = RandomOverSampler(random_state=random_seed)
X_resampled, Y_resampled = oversampler.fit_resample(X,Y)

Y_resampled.value_counts()

x_train, x_test, y_train, y_test = train_test_split(X_resampled,Y_resampled,test_size=0.2, random_state=42)

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

# Set the log level to suppress the text output
logging.getLogger("xgboost").setLevel(logging.WARNING)
logging.getLogger("catboost").setLevel(logging.WARNING)


classifiers = [
    LogisticRegression(random_state=random_seed),
    KNeighborsClassifier(),
    SVC(random_state=random_seed),
    DecisionTreeClassifier(random_state=random_seed),
    RandomForestClassifier(random_state=random_seed),
    GradientBoostingClassifier(verbose=0,random_state=random_seed),
    AdaBoostClassifier(random_state=random_seed),
    GaussianNB(),
    MLPClassifier(random_state=random_seed),
    XGBClassifier(verbosity=0,random_state=random_seed),
    ExtraTreesClassifier(random_state=random_seed),
]


scalers = [
    StandardScaler(),
    MinMaxScaler(),
    MaxAbsScaler(),
    RobustScaler(),
    PowerTransformer(),
    PowerTransformer(),
    QuantileTransformer(),
    Normalizer(),
    Binarizer()
]

results = []
for scaler in scalers:
    for classifier in classifiers:

        pipeline = Pipeline([
            ('scaler', scaler),
            ('classifier', classifier)
        ])

        scores = cross_val_score(pipeline, X_resampled, Y_resampled, cv=5, scoring='accuracy');

        y_pred = cross_val_predict(pipeline, X_resampled, Y_resampled, cv=5)
        accuracy = accuracy_score(Y_resampled, y_pred)
        recall = recall_score(Y_resampled, y_pred, average='weighted')
        precision = precision_score(Y_resampled, y_pred, average='weighted')
        f1 = f1_score(Y_resampled, y_pred, average='weighted')

        results.append((classifier.__class__.__name__, scaler.__class__.__name__, accuracy, recall, precision, f1))

results = pd.DataFrame(results, columns=['Classifier', 'Scaler', 'Accuracy', 'Recall', 'Precision', 'F1-Score'])
results = results.sort_values(by='Recall', ascending=False)
results.head()

model_pipeline = Pipeline([
    ('scaler',Normalizer()),
    ('classifier',GradientBoostingClassifier(verbose=0))
])
model_pipeline.fit(x_train,y_train)

y_pred = model_pipeline.predict(x_test)
print(classification_report(y_test,y_pred))

joblib.dump(model_pipeline,'heart_model.sav')
results.to_csv('heart_results.csv',index=False)

"""<div style="background-color:#dfe3ee; border-radius:2px; border:#000000 solid; padding: 15px; font-size:100%; text-align:center">
    <h2 align="center" style="color:#000000;"><b>_ Chronic Disease _</b></h2>
</div>
"""

chronic_data.head()

chronic_data.drop(columns=['id'],inplace=True)
chronic_data.head()

# rename column names to make it more user-friendly
chronic_data.columns = [
    'age','blood_pressure','specific_gravity','albumin','sugar','red_blood_cells','pus_cell',
    'pus_cell_clumps','bacteria','blood_glucose_random','blood_urea','serum_creatinine','sodium',
    'potassium','haemoglobin','packed_cell_volume','white_blood_cell_count','red_blood_cell_count',
    'hypertension','diabetes_mellitus','coronary_artery_disease','appetite','peda_edema','aanemia','class'
]

chronic_data.describe().T

chronic_data.info()

# converting necessary columns to numerical type
chronic_data['packed_cell_volume']     = pd.to_numeric(chronic_data['packed_cell_volume']    , errors='coerce')
chronic_data['white_blood_cell_count'] = pd.to_numeric(chronic_data['white_blood_cell_count'], errors='coerce')
chronic_data['red_blood_cell_count']   = pd.to_numeric(chronic_data['red_blood_cell_count']  , errors='coerce')

chronic_data.head()

# Extracting categorical and numerical columns
cat_cols = [col for col in chronic_data.columns if chronic_data[col].dtype == 'object']
num_cols = [col for col in chronic_data.columns if chronic_data[col].dtype != 'object']

# looking at unique values in categorical columns
for col in cat_cols:
    print(f"{col} has {chronic_data[col].unique()} values\n")

# replace incorrect values
chronic_data['diabetes_mellitus'].replace(to_replace = {'\tno':'no','\tyes':'yes',' yes':'yes'},inplace=True)
chronic_data['coronary_artery_disease'].replace(to_replace ={'\tno':'no'},inplace=True)
chronic_data['class'].replace(to_replace = {'ckd\t': 'ckd', 'notckd': 'not ckd'},inplace=True)

chronic_data['class'] = chronic_data['class'].map({'ckd': 0, 'not ckd': 1})
chronic_data['class'] = pd.to_numeric(chronic_data['class'], errors='coerce')

cols = ['diabetes_mellitus', 'coronary_artery_disease', 'class']

for col in cols:
    print(f"{col} has {chronic_data[col].unique()} values\n")

plt.figure(figsize = (20,10))
plot_number = 1

for column in num_cols:
    if plot_number <= 14:
        ax = plt.subplot(3, 5, plot_number)
        sns.distplot(chronic_data[column])
        plt.xlabel(column)

    plot_number += 1

plt.tight_layout()
plt.show()

plot_correlation_heatmap(chronic_data)

chronic_data.columns

# checking for null values
chronic_data.isna().sum().sort_values(ascending = False)

# filling null values, we will use two methods, random sampling for higher null values and  mean/mode sampling for lower null values
def random_value_imputation(feature):
    random_sample       = chronic_data[feature].dropna().sample(chronic_data[feature].isna().sum())
    random_sample.index = chronic_data[chronic_data[feature].isnull()].index
    chronic_data.loc[chronic_data[feature].isnull(), feature] = random_sample

def impute_mode(feature):
    mode = chronic_data[feature].mode()[0]
    chronic_data[feature] = chronic_data[feature].fillna(mode)

# filling num_cols null values using random sampling method
for col in num_cols:
    random_value_imputation(col)

chronic_data[num_cols].isnull().sum()

# filling "red_blood_cells" and "pus_cell" using random sampling method and rest of cat_cols using mode imputation

random_value_imputation('red_blood_cells')
random_value_imputation('pus_cell')

for col in cat_cols:
    impute_mode(col)

chronic_data[cat_cols].isnull().sum()

for col in cat_cols:
    print(f"{col} has {chronic_data[col].nunique()} categories\n")

le = LabelEncoder()

for col in cat_cols:
    chronic_data[col] = le.fit_transform(chronic_data[col])

chronic_data.head()

chronic_data.dtypes

ind_col = [col for col in chronic_data.columns if col != 'class']
dep_col = 'class'
print(ind_col)
print(len(ind_col))
X = chronic_data[ind_col]
Y = chronic_data[dep_col]

Y.value_counts()

# Random oversampling
oversampler = RandomOverSampler(random_state=random_seed)
X_resampled, Y_resampled = oversampler.fit_resample(X,Y)

Y_resampled.value_counts()

x_train, x_test, y_train, y_test = train_test_split(X_resampled,Y_resampled,test_size=0.2, random_state=random_seed)

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

logging.getLogger("xgboost").setLevel(logging.WARNING)
logging.getLogger("catboost").setLevel(logging.WARNING)

classifiers = [
    LogisticRegression(random_state=random_seed),
    KNeighborsClassifier(),
    SVC(random_state=random_seed),
    DecisionTreeClassifier(random_state=random_seed),
    RandomForestClassifier(random_state=random_seed),
    GradientBoostingClassifier(verbose=0,random_state=random_seed),
    AdaBoostClassifier(random_state=random_seed),
    GaussianNB(),
    MLPClassifier(random_state=random_seed),
    XGBClassifier(verbosity=0,random_state=random_seed),
    ExtraTreesClassifier(random_state=random_seed),
]


scalers = [
    StandardScaler(),
    MinMaxScaler(),
    MaxAbsScaler(),
    RobustScaler(),
    PowerTransformer(),
    PowerTransformer(),
    QuantileTransformer(),
    Normalizer(),
    Binarizer()
]

results = []
for scaler in scalers:
    for classifier in classifiers:

        pipeline = Pipeline([
            ('scaler', scaler),
            ('classifier', classifier)
        ])

        scores = cross_val_score(pipeline, X_resampled, Y_resampled, cv=5, scoring='accuracy');

        y_pred = cross_val_predict(pipeline, X_resampled, Y_resampled, cv=5)
        accuracy = accuracy_score(Y_resampled, y_pred)
        recall = recall_score(Y_resampled, y_pred, average='weighted')
        precision = precision_score(Y_resampled, y_pred, average='weighted')
        f1 = f1_score(Y_resampled, y_pred, average='weighted')

        results.append((classifier.__class__.__name__, scaler.__class__.__name__, accuracy, recall, precision, f1))

results = pd.DataFrame(results, columns=['Classifier', 'Scaler', 'Accuracy', 'Recall', 'Precision', 'F1-Score'])
results = results.sort_values(by='Recall', ascending=False)
results.head()

model_pipeline = Pipeline([
    ('scaler',MinMaxScaler()),
    ('classifier',ExtraTreesClassifier())
])
model_pipeline.fit(x_train,y_train)

y_pred = model_pipeline.predict(x_test)
print(classification_report(y_test,y_pred))

joblib.dump(model_pipeline,'chronic_model.sav')
results.to_csv('chronic_results.csv',index=False)

"""<div style="background-color:#dfe3ee; border-radius:2px; border:#000000 solid; padding: 15px; font-size:100%; text-align:center">
    <h2 align="center" style="color:#000000;"><b>_ Liver Disease _</b></h2>
</div>
"""

liver_data.columns = liver_data.columns.map(str.lower)
liver_data.head()

liver_data.info()

liver_data.describe().T

liver_data['albumin_and_globulin_ratio'].fillna(liver_data['albumin_and_globulin_ratio'].mean(), inplace=True)

liver_data.info()

# correlation between variables
sns.set()
sns.pairplot(liver_data, hue='dataset', kind='reg');

# full correlation table
liver_data.corr().style.background_gradient(cmap='viridis')

# this is simply my selection (from highly correlated features, you could also use different from pairs)
liver_data.drop(['direct_bilirubin', 'aspartate_aminotransferase', 'total_protiens', 'albumin'], axis=1, inplace=True);

plot_dataframe_distributions(liver_data,3,2)

# save skewed features
skewed_cols = ['albumin_and_globulin_ratio','total_bilirubin', 'alkaline_phosphotase', 'alamine_aminotransferase']

# Apply log1p transformation on dataframe - just selected values
for c in skewed_cols:
    liver_data[c] = liver_data[c].apply('log1p')

# Next check & fix strongly skewed features
# apply log1p transform
plt.figure(figsize=(15, 12))

for i, c in enumerate(skewed_cols):
    plt.subplot(5,2,i+1)
    sns.distplot(liver_data[c].apply(np.log1p))
    plt.title('Distribution plot for field:' + c)
    plt.xlabel('')
    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)

le = LabelEncoder()
liver_data['gender'] = le.fit_transform(liver_data['gender'])
liver_data['dataset'] = le.fit_transform(liver_data['dataset'])
liver_data.head()

liver_data['dataset'].value_counts()

X = liver_data.drop('dataset', axis=1)
Y = liver_data['dataset']

oversampler = RandomOverSampler(random_state=random_seed)
X_resampled, Y_resampled = oversampler.fit_resample(X,Y)

Y_resampled.value_counts()

x_train, x_test, y_train, y_test = train_test_split(X_resampled,Y_resampled,test_size=0.2, random_state=random_seed)

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

logging.getLogger("xgboost").setLevel(logging.WARNING)
logging.getLogger("catboost").setLevel(logging.WARNING)

classifiers = [
    LogisticRegression(random_state=random_seed),
    KNeighborsClassifier(),
    SVC(random_state=random_seed),
    DecisionTreeClassifier(random_state=random_seed),
    RandomForestClassifier(random_state=random_seed),
    GradientBoostingClassifier(verbose=0,random_state=random_seed),
    AdaBoostClassifier(random_state=random_seed),
    GaussianNB(),
    MLPClassifier(random_state=random_seed),
    XGBClassifier(verbosity=0,random_state=random_seed),
    ExtraTreesClassifier(random_state=random_seed),
]

scalers = [
    StandardScaler(),
    MinMaxScaler(),
    MaxAbsScaler(),
    RobustScaler(),
    PowerTransformer(),
    PowerTransformer(),
    QuantileTransformer(),
    Normalizer(),
    Binarizer()
]

results = []
for scaler in scalers:
    for classifier in classifiers:

        pipeline = Pipeline([
            ('scaler', scaler),
            ('classifier', classifier)
        ])

        scores = cross_val_score(pipeline, X_resampled, Y_resampled, cv=5, scoring='accuracy');

        y_pred = cross_val_predict(pipeline, X_resampled, Y_resampled, cv=5)
        accuracy = accuracy_score(Y_resampled, y_pred)
        recall = recall_score(Y_resampled, y_pred, average='weighted')
        precision = precision_score(Y_resampled, y_pred, average='weighted')
        f1 = f1_score(Y_resampled, y_pred, average='weighted')

        results.append((classifier.__class__.__name__, scaler.__class__.__name__, accuracy, recall, precision, f1))

results = pd.DataFrame(results, columns=['Classifier', 'Scaler', 'Accuracy', 'Recall', 'Precision', 'F1-Score'])
results = results.sort_values(by='Recall', ascending=False)
results.head()

model_pipeline = Pipeline([
    ('scaler',PowerTransformer()),
    ('classifier',ExtraTreesClassifier())
])
model_pipeline.fit(x_train,y_train)

y_pred = model_pipeline.predict(x_test)
print(classification_report(y_test,y_pred))

print(Y.iloc[11])
print("---------------------")
print(Y.iloc[10])
print("---------------------")
print(Y.iloc[100])
print("---------------------")
print(Y.iloc[101])

print(X.iloc[11].values)
print("---------------------")
print(X.iloc[10].values)
print("---------------------")
print(X.iloc[100].values)
print("---------------------")
print(X.iloc[101].values)

joblib.dump(model_pipeline,'liver_model.sav')
results.to_csv('liver_results.csv',index=False)

"""<div style="background-color:#dfe3ee; border-radius:2px; border:#000000 solid; padding: 15px; font-size:100%; text-align:center">
    <h2 align="center" style="color:#000000;"><b>_ Medical Diagnostics System ðŸ”¬ðŸ’™_</b></h2>
</div>
"""

class BioQuest:
    def __init__(self):
        self.diabetes_model  = None
        self.cancer_model    = None
        self.heart_model     = None
        self.chronic_model   = None
        self.liver_model     = None

    def load_diabetes_model(self, model_path):
        self.diabetes_model = joblib.load(model_path)

    def load_cancer_model(self, model_path):
        self.cancer_model = joblib.load(model_path)

    def load_heart_model(self, model_path):
        self.heart_model = joblib.load(model_path)

    def load_chronic_model(self, model_path):
        self.chronic_model = joblib.load(model_path)

    def load_liver_model(self, model_path):
        self.liver_model = joblib.load(model_path)

    def diabetes_prediction(self, input_values):
        feature_names = ['pregnancies', 'glucose', 'blood_pressure', 'skin_thickness', 'insulin',
                         'bmi', 'diabetes_pedigree_function', 'age']

        input_data = dict(zip(feature_names, input_values))
        input_df = pd.DataFrame([input_data])
        result = self.diabetes_model.predict(input_df)

        if result[0] == 0:
            return 'The person does not have diabetes.'
        else:
            return 'The person has diabetes.'

    def cancer_prediction(self, input_values):
        feature_names = ['radius_mean', 'area_mean', 'compactness_mean', 'concavity_mean',
                         'concave points_mean', 'area_worst', 'compactness_worst',
                         'concavity_worst', 'area_se', 'fractal_dimension_se', 'symmetry_worst',
                         'fractal_dimension_worst']

        input_data = dict(zip(feature_names, input_values))
        input_df = pd.DataFrame([input_data])
        result = self.cancer_model.predict(input_df)

        if result[0] == 0:
            return 'The person does not have breast cancer.'
        else:
            return 'The person has breast cancer.'

    def heart_prediction(self, input_values):
        feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
                         'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']

        input_data = dict(zip(feature_names, input_values))
        input_df = pd.DataFrame([input_data])
        result = self.heart_model.predict(input_df)

        if result[0] == 0:
            return 'The person does not have heart disease.'
        else:
            return 'The person has heart disease.'

    def chronic_prediction(self, input_values):
        feature_names = ['age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar',
                         'red_blood_cells', 'pus_cell', 'pus_cell_clumps', 'bacteria',
                         'blood_glucose_random', 'blood_urea', 'serum_creatinine', 'sodium',
                         'potassium', 'haemoglobin', 'packed_cell_volume',
                         'white_blood_cell_count', 'red_blood_cell_count', 'hypertension',
                         'diabetes_mellitus', 'coronary_artery_disease', 'appetite',
                         'peda_edema', 'aanemia']

        input_data = pd.DataFrame([input_values], columns=feature_names)
        result = self.chronic_model.predict(input_data)

        if result[0] == 0:
            return 'The person does not have a chronic disease.'
        else:
            return 'The person has a chronic disease.'

    def liver_prediction(self, input_values):
        feature_names = ['age', 'gender', 'total_bilirubin', 'alkaline_phosphotase',
                         'alamine_aminotransferase', 'albumin_and_globulin_ratio']

        input_data = dict(zip(feature_names, input_values))
        input_df = pd.DataFrame([input_data])
        result = self.liver_model.predict(input_df)

        if result[0] == 0:
            return 'The person does not have liver disease.'
        else:
            return 'The person has liver disease.'